{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A regression attempts to fit a of function to observed data to make predictions on new data.\n",
    "# A linear regression fits a straight line to observed data, attempting to demonstrate a linear relationship\n",
    "# between variables and make predictions on new data yet to be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [1.93939394]\n",
      "b = [4.73333333]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b4e1a8208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scikit-Learn to perform a basic, unvalidated linear regression on the sample of 10 dogs.\n",
    "from matplotlib import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import points\n",
    "df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Fit a line to the points\n",
    "fit = LinearRegression().fit(X, Y)\n",
    "\n",
    "# m = 1.7867224, b = -16.51923513\n",
    "m = fit.coef_.flatten()\n",
    "b = fit.intercept_.flatten()\n",
    "print(\"m = {0}\".format(m))\n",
    "print(\"b = {0}\".format(b))\n",
    "\n",
    "# show in chart\n",
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.plot(X, m*X+b) # line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.67272\n",
      "1.3878900000000005\n",
      "-0.5515000000000008\n",
      "2.5091099999999997\n",
      "-0.4302799999999998\n",
      "-1.3696699999999993\n",
      "0.6909400000000012\n",
      "-2.2484499999999983\n",
      "2.812160000000002\n",
      "-1.1272299999999973\n"
     ]
    }
   ],
   "source": [
    "# #  The residual is the numeric difference between the line and the points\n",
    "\n",
    "#  Another name for residuals are errors, because they reflect how wrong our line is in predicting the data.\n",
    "\n",
    "# Calculating the residuals for a given line and data\n",
    "\n",
    "# Import points\n",
    "points = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\").itertuples()\n",
    "\n",
    "# Test with a given line\n",
    "m = 1.93939\n",
    "b = 4.73333\n",
    "\n",
    "# calculate sum of squares\n",
    "for p in points:\n",
    "    y_actual = p.y\n",
    "    y_predict = m*p.x + b\n",
    "    residual = y_actual - y_predict\n",
    "    print(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are fitting a straight line through our 10 data points, we likely want to minimize these residuals\n",
    "# in total so there is as little of a gap as possible between the line and points.\n",
    "# But how do we measure the “total”? The best approach is to take the sum of squares,\n",
    "# which simply squares each residual, or multiplies each residual by itself, \n",
    "# and sums them. We take each actual y value and subtract from it the predicted y value taken from the line,\n",
    "# then square and sum all those differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might wonder why we have to square the residuals before summing them.\n",
    "# Why not just add them up without squaring?\n",
    "# That will not work because the negatives will cancel out the positives.\n",
    "# What if we add the absolute values, where we turn all negative values into positive values?\n",
    "# That sounds promising but absolute values are mathematically inconvenient.\n",
    "# More specifically, absolute values do not work well with Calculus derivatives \n",
    "# which we are going to use later for gradient descent. \n",
    "# This is why we choose the squared residuals as our way of totaling the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of squares = 28.096969704500005\n"
     ]
    }
   ],
   "source": [
    "# Calculating the sum of squares for a given line and data\n",
    "\n",
    "points = pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples()\n",
    "\n",
    "# Test with a given line\n",
    "m = 1.93939\n",
    "b = 4.73333\n",
    "\n",
    "sum_of_squares = 0.0\n",
    "\n",
    "# calculate sum of squares\n",
    "for p in points:\n",
    "    y_actual = p.y\n",
    "    y_predict = m*p.x + b\n",
    "    residual_squared = (y_predict - y_actual)**2\n",
    "    sum_of_squares += residual_squared\n",
    "\n",
    "    \n",
    "print(\"sum of squares = {}\".format(sum_of_squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9393939393939394 4.7333333333333325\n"
     ]
    }
   ],
   "source": [
    "# Calculating m and b for a simple linear regression\n",
    "\n",
    "# Load the data\n",
    "points = list(pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\").itertuples())\n",
    "\n",
    "n = len(points)\n",
    "\n",
    "m = (n*sum(p.x*p.y for p in points) - sum(p.x for p in points) *\n",
    "    sum(p.y for p in points)) / (n*sum(p.x**2 for p in points) -\n",
    "    sum(p.x for p in points)**2)\n",
    "\n",
    "b = (sum(p.y for p in points) / n) - m * sum(p.x for p in points) / n\n",
    "\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93939394 4.73333333]\n",
      "[ 6.67272727  8.61212121 10.55151515 12.49090909 14.43030303 16.36969697\n",
      " 18.30909091 20.24848485 22.18787879 24.12727273]\n"
     ]
    }
   ],
   "source": [
    "# Using inverse and transposed matrices to fit a linear regression\n",
    "\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv,qr\n",
    "import numpy as np\n",
    "\n",
    "# Import points\n",
    "df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1].flatten()\n",
    "\n",
    "# Add placeholder \"1\" column to generate intercept\n",
    "X_1 = np.vstack([X, np.ones(len(X))]).T\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Calculate coefficents for slope and intercept\n",
    "b = inv(X_1.transpose() @ X_1) @ (X_1.transpose() @ Y)\n",
    "print(b) # [1.93939394 4.73333333]\n",
    "\n",
    "# Predict against the y-values\n",
    "y_predict = X_1.dot(b)\n",
    "\n",
    "print (y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93939394 4.73333333]\n"
     ]
    }
   ],
   "source": [
    "# Using QR decomposition to perform a linear regression\n",
    "\n",
    "# Import points\n",
    "df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1].flatten()\n",
    "\n",
    "# Add placeholder \"1\" column to generate intercept\n",
    "X_1 = np.vstack([X, np.ones(len(X))]).transpose()\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# calculate coefficents for slope and intercept\n",
    "# using QR decomposition\n",
    "Q, R = qr(X_1)\n",
    "b = inv(R).dot(Q.transpose()).dot(Y)\n",
    "\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.000000000000111 4.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent is an optimization technique that uses derivatives and\n",
    "# iterations to minimize/maximize a set of parameters against an objective.\n",
    "\n",
    "# Using gradient descent to find the minimum of a parabola\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return (x - 3) ** 2 + 4\n",
    "\n",
    "def dx_f(x):\n",
    "    return 2*(x - 3)\n",
    "\n",
    "# The learning rate\n",
    "L = 0.001\n",
    "\n",
    "# The number of iterations to perform gradient descent\n",
    "iterations = 100_000\n",
    "\n",
    " # start at a random x\n",
    "x = random.randint(-15,15)\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # get slope\n",
    "    d_x = dx_f(x)\n",
    "\n",
    "    # update x by subtracting the (learning rate) * (slope)\n",
    "    x -= L * d_x\n",
    "\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1.9393939393939548x + 4.733333333333227\n"
     ]
    }
   ],
   "source": [
    "# Performing gradient descent for a linear regression\n",
    "\n",
    "# Import points from CSV\n",
    "points = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples())\n",
    "\n",
    "# Building the model\n",
    "m = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# The learning Rate\n",
    "L = .001\n",
    "\n",
    "# The number of iterations\n",
    "iterations = 100_000\n",
    "\n",
    "n = float(len(points))  # Number of elements in X\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(iterations):\n",
    "\n",
    "    # slope with respect to m\n",
    "    D_m = sum(2 * p.x * ((m * p.x + b) - p.y) for p in points)\n",
    "\n",
    "    # slope with respect to b\n",
    "    D_b = sum(2 * ((m * p.x + b) - p.y) for p in points)\n",
    "\n",
    "    # update m and b\n",
    "    m -= L * D_m\n",
    "    b -= L * D_b\n",
    "\n",
    "print(\"y = {0}x + {1}\".format(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum(2*(b + m*x(i) - y(i))*x(i), (i, 0, n))\n",
      "Sum(2*b + 2*m*x(i) - 2*y(i), (i, 0, n))\n"
     ]
    }
   ],
   "source": [
    "# Calculating partial derivatives for m and b\n",
    "\n",
    "from sympy import *\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "sum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n",
    "\n",
    "d_m = diff(sum_of_squares, m)\n",
    "d_b = diff(sum_of_squares, b)\n",
    "print(d_m)\n",
    "print(d_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.006 0.002\n",
      "10000 2.36326760682638 1.8835518624103165\n",
      "20000 2.208478682839455 2.860757272083633\n",
      "30000 2.0990271430034566 3.5015104981731477\n",
      "40000 2.059718240314351 3.918670691928773\n",
      "50000 2.0254319787495616 4.228272179662717\n",
      "60000 2.0128706983709828 4.3987557816635325\n",
      "70000 1.9588699155518574 4.511397889311558\n",
      "80000 1.9572242882073074 4.573105411192914\n",
      "90000 1.9603079930214902 4.646560110817617\n",
      "100000 1.915286401953193 4.679197268853813\n",
      "110000 1.9403467582673453 4.694118062444541\n",
      "120000 1.9599274086298113 4.725943861094068\n",
      "130000 1.9368722399981075 4.7143432848607825\n",
      "140000 1.963520934610125 4.71478629302077\n",
      "150000 1.9360986672130869 4.728305205329085\n",
      "160000 1.9324433155776284 4.741555567620757\n",
      "170000 1.926229705774608 4.7507334297890695\n",
      "180000 1.9432750003962185 4.731457564467172\n",
      "190000 1.9652083911680416 4.750695486563088\n",
      "200000 1.924857106440728 4.741023799177265\n",
      "210000 1.949168382522107 4.742302871466551\n",
      "220000 1.9482919821971325 4.732498280457749\n",
      "230000 1.9436648255769589 4.744293925905611\n",
      "240000 1.9705030140252286 4.742529291867208\n",
      "250000 1.9353429418031627 4.739497162984329\n",
      "260000 1.9562758398526634 4.738933645814807\n",
      "270000 1.9382884118185948 4.728545301878929\n",
      "280000 1.93255522206419 4.720959425792094\n",
      "290000 1.9410867423424232 4.719485579197047\n",
      "300000 1.928348444769958 4.709229998935416\n",
      "310000 1.944704859029115 4.734245656426094\n",
      "320000 1.9315489900068277 4.7164345649351676\n",
      "330000 1.939995611167781 4.709338225607842\n",
      "340000 1.961142019124275 4.705429525526002\n",
      "350000 1.9216025853518108 4.723954582852459\n",
      "360000 1.944900362530313 4.7252435418819125\n",
      "370000 1.9399721319259282 4.734938065834019\n",
      "380000 1.9497781564660994 4.742384445451385\n",
      "390000 1.9805381395468729 4.731931988681261\n",
      "400000 1.9454482978428054 4.730618875951833\n",
      "410000 1.9703383827976337 4.733665784535996\n",
      "420000 1.981650785600547 4.742295189046688\n",
      "430000 1.972530745570693 4.74071158972351\n",
      "440000 1.9111024810843704 4.726256650760091\n",
      "450000 1.9082781943786304 4.731152175860961\n",
      "460000 1.9204477114790428 4.739821573687611\n",
      "470000 1.9644981269863893 4.745855875675362\n",
      "480000 1.9263137432887223 4.743643371721059\n",
      "490000 1.91711757641405 4.74294635046912\n",
      "500000 1.930934583192176 4.74212184317665\n",
      "510000 1.967479133941378 4.736673843592278\n",
      "520000 1.9505228605459906 4.728075419769489\n",
      "530000 1.9516554472517902 4.728522913632032\n",
      "540000 1.9417919849569742 4.749508598760058\n",
      "550000 1.9611385059437294 4.766887931996634\n",
      "560000 1.9267487723441967 4.745625753029247\n",
      "570000 1.9160704416577639 4.735761047329838\n",
      "580000 1.929617903629193 4.721682821898547\n",
      "590000 1.9149289269251695 4.728326533479536\n",
      "600000 1.931097863515108 4.706517870443267\n",
      "610000 1.938513500360612 4.72158595083187\n",
      "620000 1.9654752270010292 4.727653810731125\n",
      "630000 1.9124274374663552 4.714167893880229\n",
      "640000 1.9460836587073391 4.721447016138792\n",
      "650000 1.9548197285647888 4.714751503819112\n",
      "660000 1.946696243788322 4.7234709202203105\n",
      "670000 1.943371459630136 4.722904788603818\n",
      "680000 1.9573802094954733 4.7256764624636\n",
      "690000 1.931115907669294 4.718575794959121\n",
      "700000 1.9262178442406925 4.7242553744409905\n",
      "710000 1.9584890714298269 4.707751870061937\n",
      "720000 1.91096125496631 4.732834014230203\n",
      "730000 1.9638450307901556 4.730820586569408\n",
      "740000 1.9376318945047226 4.742484193616451\n",
      "750000 1.9764575310410655 4.721173739823862\n",
      "760000 1.959201460128207 4.728725027441086\n",
      "770000 1.9616063379508497 4.745386968448955\n",
      "780000 1.9303406405244505 4.7324681138365285\n",
      "790000 1.9424228463604172 4.728831976971964\n",
      "800000 1.9043294280432401 4.69159602664981\n",
      "810000 1.9336889001186668 4.7071650308872695\n",
      "820000 1.9441607414771653 4.712772819341543\n",
      "830000 1.959822283009035 4.73382719635445\n",
      "840000 1.9453503711741207 4.724613103589071\n",
      "850000 1.9587792297040645 4.737822839830986\n",
      "860000 1.9157005293366143 4.722802115138588\n",
      "870000 1.9464146899956396 4.719103639727119\n",
      "880000 1.9111757048916815 4.714647715026774\n",
      "890000 1.9257806715010586 4.731873835908689\n",
      "900000 1.9534173041959169 4.722614847513641\n",
      "910000 1.9505340945508731 4.73813717689114\n",
      "920000 1.9330475833082221 4.73912605385538\n",
      "930000 1.9594103111947985 4.752444875710609\n",
      "940000 1.9415876754302852 4.754499178310029\n",
      "950000 1.9306788443386962 4.752825434666956\n",
      "960000 1.921548945614283 4.76041767073736\n",
      "970000 1.962487828671824 4.772474403778124\n",
      "980000 1.9865264430013605 4.754946770865421\n",
      "990000 1.9421834301828678 4.7535420839260265\n",
      "y = 1.9502363279513883x + 4.748196684882565\n"
     ]
    }
   ],
   "source": [
    "# Performing stochastic gradient descent for a linear regression\n",
    "\n",
    "\n",
    "# Input data\n",
    "data = pd.read_csv('https://bit.ly/2KF29Bd', header=0)\n",
    "\n",
    "X = data.iloc[:, 0].values\n",
    "Y = data.iloc[:, 1].values\n",
    "\n",
    "n = data.shape[0]  # rows\n",
    "\n",
    "# Building the model\n",
    "m = 0.0\n",
    "b = 0.0\n",
    "\n",
    "sample_size = 1  # sample size\n",
    "L = .0001  # The learning Rate\n",
    "epochs = 1_000_000  # The number of iterations to perform gradient descent\n",
    "\n",
    "# Performing Stochastic Gradient Descent\n",
    "for i in range(epochs):\n",
    "    idx = np.random.choice(n, sample_size, replace=False)\n",
    "    x_sample = X[idx]\n",
    "    y_sample = Y[idx]\n",
    "\n",
    "    # The current predicted value of Y\n",
    "    Y_pred = m * x_sample + b\n",
    "\n",
    "    # d/dm derivative of loss function\n",
    "    D_m = (-2 / sample_size) * sum(x_sample * (y_sample - Y_pred))\n",
    "\n",
    "    # d/db derivative of loss function\n",
    "    D_b = (-2 / sample_size) * sum(y_sample - Y_pred)\n",
    "    m = m - L * D_m  # Update m\n",
    "    b = b - L * D_b  # Update b\n",
    "\n",
    "    # print progress\n",
    "    if i % 10000 == 0:\n",
    "        print(i, m, b)\n",
    "\n",
    "print(\"y = {0}x + {1}\".format(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation coefficient, also called the Pearson correlation,\n",
    "# which measures the strength of the relationship between two variables as a value \n",
    "# between -1 and 1. A correlation coefficient closer to 0 indicates there is no correlation.\n",
    "# A correlation coefficient closer to 1 indicates a strong positive correlation,\n",
    "# meaning when one variable increases the other proportionally increases.\n",
    "# If it is closer to -1 then it indicates a strong negative correlation,\n",
    "# which means as one variable increases the other proportionally decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y\n",
      "x  1.000000  0.957586\n",
      "y  0.957586  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Using Pandas to see the correlation coefficent between every pair of variables\n",
    "\n",
    "# Read data into Pandas dataframe\n",
    "df = pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\")\n",
    "\n",
    "# Print correlations between variables\n",
    "correlations = df.corr(method='pearson')\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9575860952087218\n"
     ]
    }
   ],
   "source": [
    "# Calculating correlation coefficient from scratch in Python\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# Import points from CSV\n",
    "points = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples())\n",
    "n = len(points)\n",
    "\n",
    "numerator = n * sum(p.x * p.y for p in points) - \\\n",
    "            sum(p.x for p in points) * sum(p.y for p in points)\n",
    "\n",
    "denominator = sqrt(n*sum(p.x**2 for p in points) - sum(p.x for p in points)**2) \\\n",
    "              * sqrt(n*sum(p.y**2 for p in points) - sum(p.y for p in points)**2)\n",
    "\n",
    "corr = numerator / denominator\n",
    "\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.262157162740992 2.2621571627409915\n"
     ]
    }
   ],
   "source": [
    "#  Calculating the critical value from a T-distribution\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "n = 10\n",
    "lower_cv = t(n-1).ppf(.025)\n",
    "upper_cv = t(n-1).ppf(.975)\n",
    "\n",
    "print(lower_cv, upper_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST VALUE: 9.399564671312076\n",
      "CRITICAL RANGE: -2.262157162740992, 2.2621571627409915\n",
      "CORRELATION PROVEN, REJECT H0\n",
      "P-VALUE: 5.9763860877914965e-06\n"
     ]
    }
   ],
   "source": [
    "# Testing significance for linear-looking data\n",
    "\n",
    "from scipy.stats import t\n",
    "from math import sqrt\n",
    "\n",
    "# sample size\n",
    "n = 10\n",
    "\n",
    "lower_cv = t(n-1).ppf(.025)\n",
    "upper_cv = t(n-1).ppf(.975)\n",
    "\n",
    "# correlation coefficient\n",
    "# derived from data https://bit.ly/2KF29Bd\n",
    "r = 0.957586\n",
    "\n",
    "# Perform the test\n",
    "test_value = r / sqrt((1-r**2) / (n-2))\n",
    "\n",
    "print(\"TEST VALUE: {}\".format(test_value))\n",
    "print(\"CRITICAL RANGE: {}, {}\".format(lower_cv, upper_cv))\n",
    "\n",
    "if test_value < lower_cv or test_value > upper_cv:\n",
    "    print(\"CORRELATION PROVEN, REJECT H0\")\n",
    "else:\n",
    "    print(\"CORRELATION NOT PROVEN, FAILED TO REJECT H0 \")\n",
    "\n",
    "# Calculate p-value\n",
    "if test_value > 0:\n",
    "    p_value = 1.0 - t(n-1).cdf(test_value)\n",
    "else:\n",
    "    p_value = t(n-1).cdf(test_value)\n",
    "\n",
    "# Two-tailed, so multiply by 2\n",
    "p_value = p_value * 2\n",
    "print(\"P-VALUE: {}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y\n",
      "x  1.000000  0.916971\n",
      "y  0.916971  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Creating a correlation matrix in Pandas\n",
    "\n",
    "# Read data into Pandas dataframe\n",
    "df = pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\")\n",
    "\n",
    "# Print correlations between variables\n",
    "coeff_determination = df.corr(method='pearson') ** 2\n",
    "print(coeff_determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.462516875955465 25.966483124044537\n"
     ]
    }
   ],
   "source": [
    "# Calculating a prediction interval of vet visits for a dog that’s 8.5 years old\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# Load the data\n",
    "points = list(pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\").itertuples())\n",
    "\n",
    "n = len(points)\n",
    "\n",
    "# Linear Regression Line\n",
    "m = 1.939\n",
    "b = 4.733\n",
    "\n",
    "# Calculate Prediction Interval for x = 8.5\n",
    "x_0 = 8.5\n",
    "x_mean = sum(p.x for p in points) / len(points)\n",
    "\n",
    "t_value = t(n - 2).ppf(.975)\n",
    "\n",
    "standard_error = sqrt(sum((p.y - (m * p.x + b)) ** 2 for p in points) / (n - 2))\n",
    "\n",
    "margin_of_error = t_value * standard_error * \\\n",
    "                  sqrt(1 + (1 / n) + (n * (x_0 - x_mean) ** 2) / \\\n",
    "                       (n * sum(p.x ** 2 for p in points) - sum(p.x for p in points) ** 2))\n",
    "\n",
    "predicted_y = m*x_0 + b\n",
    "\n",
    "# Calculate prediction interval\n",
    "print(predicted_y - margin_of_error, predicted_y + margin_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.993\n"
     ]
    }
   ],
   "source": [
    "# Doing a train/test split on linear regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('https://bit.ly/3cIH97A', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Separate training and testing data\n",
    "# This leaves a third of the data out for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"R^2: %.3f\" % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99337354 0.99345032 0.99251425]\n",
      "MSE: mean=0.993 (stdev-0.000)\n"
     ]
    }
   ],
   "source": [
    "# Using 3-fold cross validation for a linear regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "df = pd.read_csv('https://bit.ly/3cIH97A', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\\\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Perform a simple linear regression\n",
    "kfold = KFold(n_splits=3, random_state=7, shuffle=True)\n",
    "model = LinearRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results)\n",
    "print(\"MSE: mean=%.3f (stdev-%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99337354 0.99345032 0.99251425]\n",
      "MSE: mean=0.993 (stdev-0.000)\n"
     ]
    }
   ],
   "source": [
    "# Using a random-fold validation for a linear regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "df = pd.read_csv('https://bit.ly/3cIH97A', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\\\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Perform a simple linear regression\n",
    "kfold = KFold(n_splits=3, random_state=7, shuffle=True)\n",
    "model = LinearRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results)\n",
    "print(\"MSE: mean=%.3f (stdev-%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients = [2.00672647 3.00203798]\n",
      "Intercept = 20.109432820035963\n",
      "z = 20.109432820035963 + 2.0067264725128062x + 3.002037976646693y\n"
     ]
    }
   ],
   "source": [
    "#  A linear regressoin with two input variables\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('https://bit.ly/2X1HWH7', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\\\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Training\n",
    "fit = LinearRegression().fit(X, Y)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients = {0}\".format(fit.coef_))\n",
    "print(\"Intercept = {0}\".format(fit.intercept_))\n",
    "print(\"z = {0} + {1}x + {2}y\".format(fit.intercept_, fit.coef_[0], fit.coef_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
